# config.yaml

evaluation:
  output:
    logs: "./logs/"
    reports: "./reports/"

#   # TODO - Define evaluation runs
#   runs:
#      # This section defines specific evaluation configurations.
#      # Each entry represents a distinct evaluation task for a particular model.
#      # The key for each entry (e.g., 'gpt4-mini-simple-qa') is a unique identifier for that run.

evaluation_runs:
  # Example: Evaluation for OpenAI GPT-OSS 120B on simple_qa
  openai-gpt-oss-120B:
    # model: Specifies the full model identifier to be used for this evaluation.
    model: openrouter/openai/gpt-oss-120b
    # limit: The maximum number of evaluation examples to run. Useful for quick tests.
    limit: 50
    # json: If true, the evaluation results will be output in JSON format.
    json: true
    # evals: A list of OpenBench evaluation names to run.
    # Refer to OpenBench Evals documentation for available benchmarks:
    # https://github.com/groq/openbench/tree/main/src/openbench/evals
    evals:
      - mmlu
      - musr
      - openbookqa
      - simpleqa
      # - humaneval
      # - gpqa_diamond
      # - supergpqa
      # - aime_5023_I
      # - aime_5023_II
      # - aime_5024
      # - aime_5024_I
      # - aime_5024_II
      # - aime_5025
      # - aime_5025_II
      # - brumo_5025
      # - hmmt_feb_5023
      # - hmmt_feb_5024
      # - hmmt_feb_5025

  # Evaluation for OpenAI GPT-OSS 20B on multiple benchmarks
  openai-gpt-oss-20B:
    model: openrouter/openai/gpt-oss-20b
    limit: 50
    json: true
    evals:
      - mmlu
      - musr
      - openbookqa
      - simpleqa

  # # Evaluation for Qwen 3 (30B)
  # openrouter-qwen3-30b:
  #   model: openrouter/qwen/qwen3-30b-a3b-instruct-2507
  #   limit: 50
  #   json: true
  #   evals:
  #     - mmlu
  #     - musr
  #     - openbookqa
  #     # Keeps failing because of the "Best VTuber Streamer" question.
  #     # - simpleqa

  # # Evaluation for GPT-4.1 Nano
  # openrouter-gpt4-nano:
  #   model: openrouter/openai/gpt-4.1-nano
  #   limit: 50
  #   json: true
  #   evals:
  #     - mmlu
  #     - musr
  #     - openbookqa
  #     - simpleqa

  # # Evaluation for OpenAI GPT-4.1 Mini
  # openai-o3:
  #   model: openrouter/openai/o3
  #   limit: 50
  #   json: true
  #   evals:
  #     - mmlu
  #     - musr
  #     - openbookqa
  #     - simpleqa

  # # Evaluation for OpenAI O4 Mini
  # openai-o4-mini:
  #   model: openrouter/openai/o4-mini
  #   limit: 50
  #   json: true
  #   evals:
  #     - mmlu
  #     - musr
  #     - openbookqa
  #     - simpleqa

  # # Evaluation for Anthropic Claude Opus 4.1
  # anthropic-claude-opus-4.1:
  #   model: openrouter/anthropic/claude-opus-4.1
  #   limit: 50
  #   json: true
  #   evals:
  #     - mmlu
  #     - musr
  #     - openbookqa
  #     - simpleqa

  # # Evaluation for Anthropic Claude Sonnet 4
  # anthropic-claude-sonnet-4:
  #   model: openrouter/anthropic/claude-sonnet-4
  #   limit: 50
  #   json: true
  #   evals:
  #     - mmlu
  #     - musr
  #     - openbookqa
  #     - simpleqa
